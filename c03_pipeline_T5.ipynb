{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtP9rEPFrxN2yWtVrMcDGy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from transformers import pipeline\n","import sentencepiece\n","\n","import torch"],"metadata":{"id":"DBtPSyYP0Vr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = [0 if torch.cuda.is_available() else 'cpu'][0]\n","device"],"metadata":{"id":"uSzD4FIC26Ay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=\"anshoomehra/t5-v1-base-s-q-c-multi-task-qgen-v2\"\n","\n","modelo = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)\n","modelo_tokenizer= AutoTokenizer.from_pretrained(model)"],"metadata":{"id":"KIQ4-3Ri0Zuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modelo_T5_pipeline(question, context):    \n","    \n","    input_text = \"question: \" + question + \"</s> question_context: \" + context\n","    \n","    input_tokenized = modelo_tokenizer.encode(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=1024).to(device)\n","    \n","    summary_ids = modelo.generate(input_tokenized, \n","                                       max_length=100, \n","                                       min_length=5, \n","                                       num_beams=4,\n","                                       early_stopping=True)\n","    \n","    output = [modelo_tokenizer.decode(id, clean_up_tokenization_spaces=True, skip_special_tokens=True) for id in summary_ids] \n","    \n","    return str(output[0])"],"metadata":{"id":"mf9B45c70wUo"},"execution_count":null,"outputs":[]}]}