{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8At/XYtsesEeQUbgSuasa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IAjGhEN3GTyi"},"outputs":[],"source":["# To scrape Wikipedia\n","from bs4 import BeautifulSoup\n","# To access contents from URLs\n","import requests\n","import re"]},{"cell_type":"code","source":["def procesamiento_tema(tema): \n","\n","  # Tema del ChatBot\n","  # tema = tema.lower().strip().title().replace(' ', '_')\n","\n","  tema = tema.strip().replace(' ', '_')\n","\n","  # Link de Wikipedia\n","  link = 'https://en.wikipedia.org/wiki/'+ tema\n","\n","  # Información del link de wikipedia\n","  url_contenido= requests.get(link).content\n","  soup_tema = BeautifulSoup(url_contenido, 'html.parser')\n","\n","  # Encontrar solo el texto de wikipedia\n","  p_texto = soup_tema.findAll('p')\n","\n","  # Concatenar en texto\n","  texto = ''\n","\n","  for t in p_texto:\n","\n","    rm_style = t.findAll('style')\n","\n","    for m in rm_style:\n","      m.decompose()\n","\n","    t = t.stripped_strings\n","    t = ' '.join(list(t))\n","    t = t.replace(r'( ','(').replace(' )', ')').replace(' .', '.').replace(' ,', ',')\n","    t = re.sub(u'\\[.*\\u200b', '', t)\n","\n","    texto = texto + ' '+ t\n","\n","  # últimos reemplazos en texto\n","\n","  replace = [(r'[;»«\"]', ''),\n","            (r' \\[\\d{1,}\\]', ''), \n","            (r' \\[[a-z]{1,}\\]', ''),  \n","            (r'\\s+',' '), \n","            (r'\\(GMT.*\\)', ''),\n","            (r'^\\s', '')]\n","\n","  for old, new in replace:\n","      texto = re.sub(old, new, texto) \n","\n","  return texto"],"metadata":{"id":"4omxuqYoGYZR"},"execution_count":null,"outputs":[]}]}